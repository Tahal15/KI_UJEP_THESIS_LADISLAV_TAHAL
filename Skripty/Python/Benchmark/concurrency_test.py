import time
import concurrent.futures
import statistics
import pandas as pd
import matplotlib.pyplot as plt
import pymysql
import pyodbc
import psycopg2
from clickhouse_driver import Client
from datetime import datetime

# ==============================================================================
# üîß KONFIGURACE P≈òIPOJEN√ç
# ==============================================================================

DB_CONFIG = {
    "MSSQL": {
        "driver": "{ODBC Driver 17 for SQL Server}",
        "server": "TAHAL\\DATA_WAREHOUSE",
        "database": "DWH",
        "uid": "sa",
        "pwd": "HesloProBakalarku2025*"
    },
    "MariaDB": {
        "host": "localhost",
        "port": 3308, # DWH port
        "user": "admin",
        "password": "C0lumnStore!",
        "database": "mttgueries"
    },
    "MariaDB_InnoDB": {
        "host": "localhost",
        "port": 3308, # DWH port
        "user": "admin",
        "password": "C0lumnStore!",
        "database": "mttgueries"
    },
    "ClickHouse": {
        "host": "localhost",
        "port": 9000,
        "user": "tahal",
        "password": "tohlejeroothesloprobakalarku2025",
        "database": "default"
    },
    "PostgreSQL": {
        "host": "localhost",
        "port": 5434, # DWH port
        "user": "tahal",
        "password": "tohlejeroothesloprobakalarku2025",
        "database": "datovy_sklad"
    }
}

# ==============================================================================
# üìù SQL DOTAZY (Dialekty)
# ==============================================================================

QUERIES = {
    "MSSQL": [
        # Q1: Jednoduch√° agregace
        """
        SELECT vc.VehicleClass, AVG(f.Velocity) AS AverageVelocity
        FROM FactCameraDetection f
        JOIN DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        GROUP BY vc.VehicleClass;
        """,
        # Q2: Agregace p≈ôes ƒças
        """
        SELECT t.FullDate, COUNT(*) AS NumberOfDetections
        FROM FactCameraDetection f
        JOIN DimTime t ON f.TimeKey = t.TimeKey
        GROUP BY t.FullDate
        ORDER BY t.FullDate;
        """,
        # Q3: Top 10 Mƒõsta
        """
        SELECT TOP 10 c.CityName, AVG(f.Velocity) AS AverageVelocity
        FROM FactCameraDetection f
        JOIN DimCity c ON f.CityKey = c.CityKey
        JOIN DimCountry co ON f.CountryKey = co.CountryKey
        JOIN DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        WHERE co.CountryCode = 'CZ' AND vc.VehicleClass = '2'
        GROUP BY c.CityName
        ORDER BY AverageVelocity DESC;
        """,
        # Q4: 2 Dimenze
        """
        SELECT dt.DetectionType, vc.VehicleClass, AVG(f.Velocity) AS AverageVelocity
        FROM FactCameraDetection f
        JOIN DimDetectionType dt ON f.DetectionTypeKey = dt.DetectionTypeKey
        JOIN DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        GROUP BY dt.DetectionType, vc.VehicleClass
        ORDER BY dt.DetectionType, vc.VehicleClass;
        """,
        # Q5: Komplexn√≠ (V√≠kendy)
        """
        WITH WeekendDetections AS (
            SELECT f.CityKey, f.CountryKey, f.VehicleClassKey, f.Velocity
            FROM FactCameraDetection f
            JOIN DimTime t ON f.TimeKey = t.TimeKey
            WHERE DATEPART(weekday, t.FullDate) IN (1, 7) -- Nedƒõle=1, Sobota=7 (default US)
        )
        SELECT co.CountryCode, c.CityName, vc.VehicleClass, 
               AVG(wd.Velocity) AS AverageVelocity,
               DENSE_RANK() OVER(PARTITION BY co.CountryCode ORDER BY COUNT(*) DESC) AS CityRankByDetections
        FROM WeekendDetections wd
        JOIN DimCity c ON wd.CityKey = c.CityKey
        JOIN DimCountry co ON wd.CountryKey = co.CountryKey
        JOIN DimVehicleClass vc ON wd.VehicleClassKey = vc.VehicleClassKey
        GROUP BY co.CountryCode, c.CityName, vc.VehicleClass
        ORDER BY co.CountryCode, CityRankByDetections;
        """
    ],
    "MariaDB": [
        # Q1
        """
        SELECT vc.VehicleClass, AVG(f.Velocity) AS AverageVelocity
        FROM FactCameraDetection f
        JOIN DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        GROUP BY vc.VehicleClass;
        """,
        # Q2
        """
        SELECT t.FullDate, COUNT(*) AS NumberOfDetections
        FROM FactCameraDetection f
        JOIN DimTime t ON f.TimeKey = t.TimeKey
        GROUP BY t.FullDate
        ORDER BY t.FullDate;
        """,
        # Q3 (LIMIT m√≠sto TOP)
        """
        SELECT c.CityName, AVG(f.Velocity) AS AverageVelocity
        FROM FactCameraDetection f
        JOIN DimCity c ON f.CityKey = c.CityKey
        JOIN DimCountry co ON f.CountryKey = co.CountryKey
        JOIN DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        WHERE co.CountryCode = 'CZ' AND vc.VehicleClass = '2'
        GROUP BY c.CityName
        ORDER BY AverageVelocity DESC
        LIMIT 10;
        """,
        # Q4
        """
        SELECT dt.DetectionType, vc.VehicleClass, AVG(f.Velocity) AS AverageVelocity
        FROM FactCameraDetection f
        JOIN DimDetectionType dt ON f.DetectionTypeKey = dt.DetectionTypeKey
        JOIN DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        GROUP BY dt.DetectionType, vc.VehicleClass
        ORDER BY dt.DetectionType, vc.VehicleClass;
        """,
        # Q5 (DAYOFWEEK: 1=Sun, 7=Sat)
        """
        WITH WeekendDetections AS (
            SELECT f.CityKey, f.CountryKey, f.VehicleClassKey, f.Velocity
            FROM FactCameraDetection f
            JOIN DimTime t ON f.TimeKey = t.TimeKey
            WHERE DAYOFWEEK(t.FullDate) IN (1, 7)
        )
        SELECT co.CountryCode, c.CityName, vc.VehicleClass, 
               AVG(wd.Velocity) AS AverageVelocity,
               DENSE_RANK() OVER(PARTITION BY co.CountryCode ORDER BY COUNT(*) DESC) AS CityRankByDetections
        FROM WeekendDetections wd
        JOIN DimCity c ON wd.CityKey = c.CityKey
        JOIN DimCountry co ON wd.CountryKey = co.CountryKey
        JOIN DimVehicleClass vc ON wd.VehicleClassKey = vc.VehicleClassKey
        GROUP BY co.CountryCode, c.CityName, vc.VehicleClass
        ORDER BY co.CountryCode, CityRankByDetections;
        """
    ],
    "MariaDB_InnoDB": [
        # Q1
        """
        SELECT vc.VehicleClass, AVG(f.Velocity) AS AverageVelocity
        FROM FactCameraDetection_innodb_old f
        JOIN DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        GROUP BY vc.VehicleClass;
        """,
        # Q2
        """
        SELECT t.FullDate, COUNT(*) AS NumberOfDetections
        FROM FactCameraDetection_innodb_old f
        JOIN DimTime t ON f.TimeKey = t.TimeKey
        GROUP BY t.FullDate
        ORDER BY t.FullDate;
        """,
        # Q3 (LIMIT m√≠sto TOP)
        """
        SELECT c.CityName, AVG(f.Velocity) AS AverageVelocity
        FROM FactCameraDetection_innodb_old f
        JOIN DimCity c ON f.CityKey = c.CityKey
        JOIN DimCountry co ON f.CountryKey = co.CountryKey
        JOIN DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        WHERE co.CountryCode = 'CZ' AND vc.VehicleClass = '2'
        GROUP BY c.CityName
        ORDER BY AverageVelocity DESC
        LIMIT 10;
        """,
        # Q4
        """
        SELECT dt.DetectionType, vc.VehicleClass, AVG(f.Velocity) AS AverageVelocity
        FROM FactCameraDetection_innodb_old f
        JOIN DimDetectionType dt ON f.DetectionTypeKey = dt.DetectionTypeKey
        JOIN DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        GROUP BY dt.DetectionType, vc.VehicleClass
        ORDER BY dt.DetectionType, vc.VehicleClass;
        """,
        # Q5 (DAYOFWEEK: 1=Sun, 7=Sat)
        """
        WITH WeekendDetections AS (
            SELECT f.CityKey, f.CountryKey, f.VehicleClassKey, f.Velocity
            FROM FactCameraDetection_innodb_old f
            JOIN DimTime t ON f.TimeKey = t.TimeKey
            WHERE DAYOFWEEK(t.FullDate) IN (1, 7)
        )
        SELECT co.CountryCode, c.CityName, vc.VehicleClass, 
               AVG(wd.Velocity) AS AverageVelocity,
               DENSE_RANK() OVER(PARTITION BY co.CountryCode ORDER BY COUNT(*) DESC) AS CityRankByDetections
        FROM WeekendDetections wd
        JOIN DimCity c ON wd.CityKey = c.CityKey
        JOIN DimCountry co ON wd.CountryKey = co.CountryKey
        JOIN DimVehicleClass vc ON wd.VehicleClassKey = vc.VehicleClassKey
        GROUP BY co.CountryCode, c.CityName, vc.VehicleClass
        ORDER BY co.CountryCode, CityRankByDetections;
        """
    ],
    "ClickHouse": [
        # Q1
        """
        SELECT vc.VehicleClass, avg(f.Velocity) AS AverageVelocity
        FROM FactCameraDetection f
        JOIN DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        GROUP BY vc.VehicleClass;
        """,
        # Q2
        """
        SELECT t.FullDate, count(*) AS NumberOfDetections
        FROM FactCameraDetection f
        JOIN DimTime t ON f.TimeKey = t.TimeKey
        GROUP BY t.FullDate
        ORDER BY t.FullDate;
        """,
        # Q3
        """
        SELECT c.CityName, avg(f.Velocity) AS AverageVelocity
        FROM FactCameraDetection f
        JOIN DimCity c ON f.CityKey = c.CityKey
        JOIN DimCountry co ON f.CountryKey = co.CountryKey
        JOIN DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        WHERE co.CountryCode = 'CZ' AND vc.VehicleClass = '2'
        GROUP BY c.CityName
        ORDER BY AverageVelocity DESC
        LIMIT 10;
        """,
        # Q4
        """
        SELECT dt.DetectionType, vc.VehicleClass, avg(f.Velocity) AS AverageVelocity
        FROM FactCameraDetection f
        JOIN DimDetectionType dt ON f.DetectionTypeKey = dt.DetectionTypeKey
        JOIN DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        GROUP BY dt.DetectionType, vc.VehicleClass
        ORDER BY dt.DetectionType, vc.VehicleClass;
        """,
        # Q5 (toDayOfWeek: 1=Mon, 7=Sun -> So(6), Ne(7))
        """
        WITH WeekendDetections AS (
            SELECT f.CityKey, f.CountryKey, f.VehicleClassKey, f.Velocity
            FROM FactCameraDetection f
            JOIN DimTime t ON f.TimeKey = t.TimeKey
            WHERE toDayOfWeek(t.FullDate) IN (6, 7)
        )
        SELECT co.CountryCode, c.CityName, vc.VehicleClass, 
               avg(wd.Velocity) AS AverageVelocity,
               dense_rank() OVER(PARTITION BY co.CountryCode ORDER BY count(*) DESC) AS CityRankByDetections
        FROM WeekendDetections wd
        JOIN DimCity c ON wd.CityKey = c.CityKey
        JOIN DimCountry co ON wd.CountryKey = co.CountryKey
        JOIN DimVehicleClass vc ON wd.VehicleClassKey = vc.VehicleClassKey
        GROUP BY co.CountryCode, c.CityName, vc.VehicleClass
        ORDER BY co.CountryCode, CityRankByDetections;
        """
    ],
    "PostgreSQL": [
        # Q1
        """
        SELECT vc.VehicleClass, AVG(f.Velocity) AS AverageVelocity
        FROM mttgueries.FactCameraDetection f
        JOIN mttgueries.DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        GROUP BY vc.VehicleClass;
        """,
        # Q2
        """
        SELECT t.FullDate, COUNT(*) AS NumberOfDetections
        FROM mttgueries.FactCameraDetection f
        JOIN mttgueries.DimTime t ON f.TimeKey = t.TimeKey
        GROUP BY t.FullDate
        ORDER BY t.FullDate;
        """,
        # Q3
        """
        SELECT c.CityName, AVG(f.Velocity) AS AverageVelocity
        FROM mttgueries.FactCameraDetection f
        JOIN mttgueries.DimCity c ON f.CityKey = c.CityKey
        JOIN mttgueries.DimCountry co ON f.CountryKey = co.CountryKey
        JOIN mttgueries.DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        WHERE co.CountryCode = 'CZ' AND vc.VehicleClass = '2'
        GROUP BY c.CityName
        ORDER BY AverageVelocity DESC
        LIMIT 10;
        """,
        # Q4
        """
        SELECT dt.DetectionType, vc.VehicleClass, AVG(f.Velocity) AS AverageVelocity
        FROM mttgueries.FactCameraDetection f
        JOIN mttgueries.DimDetectionType dt ON f.DetectionTypeKey = dt.DetectionTypeKey
        JOIN mttgueries.DimVehicleClass vc ON f.VehicleClassKey = vc.VehicleClassKey
        GROUP BY dt.DetectionType, vc.VehicleClass
        ORDER BY dt.DetectionType, vc.VehicleClass;
        """,
        # Q5 (ISODOW: 1=Mon, 7=Sun -> So(6), Ne(7))
        """
        WITH WeekendDetections AS (
            SELECT f.CityKey, f.CountryKey, f.VehicleClassKey, f.Velocity
            FROM mttgueries.FactCameraDetection f
            JOIN mttgueries.DimTime t ON f.TimeKey = t.TimeKey
            WHERE EXTRACT(ISODOW FROM t.FullDate) IN (6, 7)
        )
        SELECT co.CountryCode, c.CityName, vc.VehicleClass, 
               AVG(wd.Velocity) AS AverageVelocity,
               DENSE_RANK() OVER(PARTITION BY co.CountryCode ORDER BY COUNT(*) DESC) AS CityRankByDetections
        FROM WeekendDetections wd
        JOIN mttgueries.DimCity c ON wd.CityKey = c.CityKey
        JOIN mttgueries.DimCountry co ON wd.CountryKey = co.CountryKey
        JOIN mttgueries.DimVehicleClass vc ON wd.VehicleClassKey = vc.VehicleClassKey
        GROUP BY co.CountryCode, c.CityName, vc.VehicleClass
        ORDER BY co.CountryCode, CityRankByDetections;
        """
    ]
}

# ==============================================================================
# üèÉ‚Äç‚ôÇÔ∏è BENCHMARK ENGINE
# ==============================================================================

def get_connection(db_type):
    """Vr√°t√≠ connection objekt pro dan√Ω typ datab√°ze."""
    cfg = DB_CONFIG[db_type]
    
    if db_type == "MSSQL":
        conn_str = (
            f"Driver={cfg['driver']};"
            f"Server={cfg['server']};"
            f"Database={cfg['database']};"
            f"UID={cfg['uid']};"
            f"PWD={cfg['pwd']};"
        )
        return pyodbc.connect(conn_str)
    
    elif db_type == "MariaDB" or db_type == "MariaDB_InnoDB":
        return pymysql.connect(
            host=cfg['host'],
            port=cfg['port'],
            user=cfg['user'],
            password=cfg['password'],
            database=cfg['database']
        )
    
    elif db_type == "ClickHouse":
        # ClickHouse driver pou≈æ√≠v√° Client objekt, ne standardn√≠ DB-API 2.0 connection
        return Client(
            host=cfg['host'],
            port=cfg['port'],
            user=cfg['user'],
            password=cfg['password'],
            database=cfg['database']
        )

    elif db_type == "PostgreSQL":
        conn = psycopg2.connect(
            host=cfg['host'],
            port=cfg['port'],
            user=cfg['user'],
            password=cfg['password'],
            dbname=cfg['database']
        )
        # üîß FIX: Vypnut√≠ paraleln√≠ch worker≈Ø pro tento session.
        # Na Windows p≈ôi vysok√© konkurenci (20+ users) doch√°z√≠ k vyƒçerp√°n√≠ sd√≠len√© pamƒõti ("No space left on device"),
        # pokud se ka≈æd√Ω dotaz sna≈æ√≠ spustit v√≠ce paraleln√≠ch proces≈Ø.
        try:
            with conn.cursor() as cur:
                cur.execute("SET max_parallel_workers_per_gather = 0;")
            conn.commit()
        except Exception as e:
            print(f"‚ö†Ô∏è Nepoda≈ôilo se nastavit max_parallel_workers_per_gather: {e}")
        
        return conn
    
    raise ValueError(f"Nezn√°m√Ω typ datab√°ze: {db_type}")

def execute_query(db_type, query):
    """Provede jeden dotaz a vr√°t√≠ dobu trv√°n√≠ v sekund√°ch."""
    conn = None
    start_time = time.time()
    
    try:
        conn = get_connection(db_type)
        
        if db_type == "ClickHouse":
            # ClickHouse driver
            conn.execute(query)
        else:
            # Standardn√≠ DB-API (MSSQL, MariaDB, PostgreSQL)
            cursor = conn.cursor()
            cursor.execute(query)
            # Fetch all data to ensure query is fully processed
            cursor.fetchall()
            cursor.close()
            
    except Exception as e:
        print(f"‚ùå Chyba ({db_type}): {e}")
        return None
    finally:
        if conn:
            if db_type == "ClickHouse":
                conn.disconnect()
            else:
                conn.close()
                
    end_time = time.time()
    return end_time - start_time

def run_benchmark(db_type, concurrency, num_queries=5):
    """Spust√≠ benchmark pro danou datab√°zi a √∫rove≈à konkurence."""
    print(f"üöÄ Spou≈°t√≠m test: {db_type} | U≈æivatel≈Ø: {concurrency}")
    
    queries = QUERIES[db_type]
    latencies = []
    
    # Pool vl√°ken simuluje soubƒõ≈æn√© u≈æivatele
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
        futures = []
        
        # Ka≈æd√Ω "u≈æivatel" spust√≠ sadu dotaz≈Ø
        for _ in range(concurrency):
            for q in queries:
                futures.append(executor.submit(execute_query, db_type, q))
        
        # Sbƒõr v√Ωsledk≈Ø
        for future in concurrent.futures.as_completed(futures):
            duration = future.result()
            if duration is not None:
                latencies.append(duration)
                
    if not latencies:
        print("‚ö†Ô∏è ≈Ω√°dn√© √∫spƒõ≈°n√© dotazy.")
        return None

    # Statistiky
    avg_lat = statistics.mean(latencies)
    p95_lat = statistics.quantiles(latencies, n=20)[18] # 95th percentile
    total_time = sum(latencies) # Toto je souƒçet ƒças≈Ø vl√°ken, ne wall-clock time
    # Pro throughput pot≈ôebujeme wall-clock time cel√©ho testu, ale tady to zjednodu≈°√≠me
    # Lep≈°√≠ by bylo mƒõ≈ôit ƒças vnƒõ ThreadPoolExecutoru
    
    return {
        "Database": db_type,
        "Concurrency": concurrency,
        "Avg_Latency_s": round(avg_lat, 4),
        "P95_Latency_s": round(p95_lat, 4),
        "Min_Latency_s": round(min(latencies), 4),
        "Max_Latency_s": round(max(latencies), 4),
        "Total_Queries": len(latencies)
    }

# ==============================================================================
# üìä HLAVN√ç SMYƒåKA
# ==============================================================================

import argparse
import os

# ... (imports remain the same, ensuring argparse and os are available)

# ==============================================================================
# üìä HLAVN√ç SMYƒåKA
# ==============================================================================

def main():
    parser = argparse.ArgumentParser(description="Benchmark datab√°z√≠ - soubƒõ≈æn√° z√°tƒõ≈æ")
    parser.add_argument("--db", type=str, help="Specifikujte datab√°zi k testov√°n√≠ (MSSQL, MariaDB, ClickHouse, PostgreSQL). Pokud neuvedeno, testuj√≠ se v≈°echny.", default=None)
    args = parser.parse_args()

    # Nastaven√≠ testu
    available_dbs = ["MSSQL", "MariaDB", "MariaDB_InnoDB", "ClickHouse", "PostgreSQL"]
    
    if args.db:
        if args.db not in available_dbs:
            print(f"‚ùå Nezn√°m√° datab√°ze: {args.db}. Dostupn√©: {', '.join(available_dbs)}")
            return
        databases_to_test = [args.db]
    else:
        databases_to_test = available_dbs

    concurrency_levels = [1, 5, 10, 20]
    results = []
    csv_file = "benchmark_results.csv"
    
    print("==================================================")
    print("   BENCHMARK DATAB√ÅZ√ç - SOUBƒö≈ΩN√Å Z√ÅTƒö≈Ω")
    print("==================================================")
    print(f"Testovan√© datab√°ze: {', '.join(databases_to_test)}")
    
    for db in databases_to_test:
        for users in concurrency_levels:
            start_wall_clock = time.time()
            
            stats = run_benchmark(db, users)
            
            end_wall_clock = time.time()
            total_duration = end_wall_clock - start_wall_clock
            
            if stats:
                # P≈ôid√°me throughput (Queries Per Second)
                stats["TPS"] = round(stats["Total_Queries"] / total_duration, 2)
                results.append(stats)
                print(f"   üëâ TPS: {stats['TPS']} | Avg Latency: {stats['Avg_Latency_s']}s\n")
            
            # Pauza mezi testy
            time.sleep(2)

    # Naƒçten√≠ existuj√≠c√≠ch v√Ωsledk≈Ø, pokud existuj√≠, pro kombinovan√Ω graf
    if os.path.exists(csv_file):
        try:
            existing_df = pd.read_csv(csv_file)
            # Odstran√≠me star√© v√Ωsledky pro pr√°vƒõ testovan√© datab√°ze, abychom je nahradili nov√Ωmi
            if not existing_df.empty:
                existing_df = existing_df[~existing_df['Database'].isin(databases_to_test)]
            
            # Spoj√≠me star√© a nov√© v√Ωsledky
            new_df = pd.DataFrame(results)
            final_df = pd.concat([existing_df, new_df], ignore_index=True)
        except Exception as e:
            print(f"‚ö†Ô∏è Chyba p≈ôi ƒçten√≠ existuj√≠c√≠ho CSV: {e}. Vytv√°≈ô√≠m nov√©.")
            final_df = pd.DataFrame(results)
    else:
        final_df = pd.DataFrame(results)

    print("\nüìä V√ùSLEDKY (Aktualizovan√©):")
    print(final_df)
    
        )
        return pyodbc.connect(conn_str)
    
    elif db_type == "MariaDB" or db_type == "MariaDB_InnoDB":
        return pymysql.connect(
            host=cfg['host'],
            port=cfg['port'],
            user=cfg['user'],
            password=cfg['password'],
            database=cfg['database']
        )
    
    elif db_type == "ClickHouse":
        # ClickHouse driver pou≈æ√≠v√° Client objekt, ne standardn√≠ DB-API 2.0 connection
        return Client(
            host=cfg['host'],
            port=cfg['port'],
            user=cfg['user'],
            password=cfg['password'],
            database=cfg['database']
        )

    elif db_type == "PostgreSQL":
        conn = psycopg2.connect(
            host=cfg['host'],
            port=cfg['port'],
            user=cfg['user'],
            password=cfg['password'],
            dbname=cfg['database']
        )
        # üîß FIX: Vypnut√≠ paraleln√≠ch worker≈Ø pro tento session.
        # Na Windows p≈ôi vysok√© konkurenci (20+ users) doch√°z√≠ k vyƒçerp√°n√≠ sd√≠len√© pamƒõti ("No space left on device"),
        # pokud se ka≈æd√Ω dotaz sna≈æ√≠ spustit v√≠ce paraleln√≠ch proces≈Ø.
        try:
            with conn.cursor() as cur:
                cur.execute("SET max_parallel_workers_per_gather = 0;")
            conn.commit()
        except Exception as e:
            print(f"‚ö†Ô∏è Nepoda≈ôilo se nastavit max_parallel_workers_per_gather: {e}")
        
        return conn
    
    raise ValueError(f"Nezn√°m√Ω typ datab√°ze: {db_type}")

def execute_query(db_type, query):
    """Provede jeden dotaz a vr√°t√≠ dobu trv√°n√≠ v sekund√°ch."""
    conn = None
    start_time = time.time()
    
    try:
        conn = get_connection(db_type)
        
        if db_type == "ClickHouse":
            # ClickHouse driver
            conn.execute(query)
        else:
            # Standardn√≠ DB-API (MSSQL, MariaDB, PostgreSQL)
            cursor = conn.cursor()
            cursor.execute(query)
            # Fetch all data to ensure query is fully processed
            cursor.fetchall()
            cursor.close()
            
    except Exception as e:
        print(f"‚ùå Chyba ({db_type}): {e}")
        return None
    finally:
        if conn:
            if db_type == "ClickHouse":
                conn.disconnect()
            else:
                conn.close()
                
    end_time = time.time()
    return end_time - start_time

def run_benchmark(db_type, concurrency, num_queries=5):
    """Spust√≠ benchmark pro danou datab√°zi a √∫rove≈à konkurence."""
    print(f"üöÄ Spou≈°t√≠m test: {db_type} | U≈æivatel≈Ø: {concurrency}")
    
    queries = QUERIES[db_type]
    latencies = []
    
    # Pool vl√°ken simuluje soubƒõ≈æn√© u≈æivatele
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
        futures = []
        
        # Ka≈æd√Ω "u≈æivatel" spust√≠ sadu dotaz≈Ø
        for _ in range(concurrency):
            for q in queries:
                futures.append(executor.submit(execute_query, db_type, q))
        
        # Sbƒõr v√Ωsledk≈Ø
        for future in concurrent.futures.as_completed(futures):
            duration = future.result()
            if duration is not None:
                latencies.append(duration)
                
    if not latencies:
        print("‚ö†Ô∏è ≈Ω√°dn√© √∫spƒõ≈°n√© dotazy.")
        return None

    # Statistiky
    avg_lat = statistics.mean(latencies)
    p95_lat = statistics.quantiles(latencies, n=20)[18] # 95th percentile
    total_time = sum(latencies) # Toto je souƒçet ƒças≈Ø vl√°ken, ne wall-clock time
    # Pro throughput pot≈ôebujeme wall-clock time cel√©ho testu, ale tady to zjednodu≈°√≠me
    # Lep≈°√≠ by bylo mƒõ≈ôit ƒças vnƒõ ThreadPoolExecutoru
    
    return {
        "Database": db_type,
        "Concurrency": concurrency,
        "Avg_Latency_s": round(avg_lat, 4),
        "P95_Latency_s": round(p95_lat, 4),
        "Min_Latency_s": round(min(latencies), 4),
        "Max_Latency_s": round(max(latencies), 4),
        "Total_Queries": len(latencies)
    }

# ==============================================================================
# üìä HLAVN√ç SMYƒåKA
# ==============================================================================

import argparse
import os
import seaborn as sns

# ... (imports remain the same, ensuring argparse and os are available)

# ==============================================================================
# üìä HLAVN√ç SMYƒåKA
# ==============================================================================

def main():
    parser = argparse.ArgumentParser(description="Benchmark datab√°z√≠ - soubƒõ≈æn√° z√°tƒõ≈æ")
    parser.add_argument("--db", type=str, help="Specifikujte datab√°zi k testov√°n√≠ (MSSQL, MariaDB, ClickHouse, PostgreSQL). Pokud neuvedeno, testuj√≠ se v≈°echny.", default=None)
    args = parser.parse_args()

    # Nastaven√≠ testu
    available_dbs = ["MSSQL", "MariaDB", "MariaDB_InnoDB", "ClickHouse", "PostgreSQL"]
    
    if args.db:
        if args.db not in available_dbs:
            print(f"‚ùå Nezn√°m√° datab√°ze: {args.db}. Dostupn√©: {', '.join(available_dbs)}")
            return
        databases_to_test = [args.db]
    else:
        databases_to_test = available_dbs

    concurrency_levels = [1, 5, 10, 20]
    results = []
    csv_file = "benchmark_results.csv"
    
    print("==================================================")
    print("   BENCHMARK DATAB√ÅZ√ç - SOUBƒö≈ΩN√Å Z√ÅTƒö≈Ω")
    print("==================================================")
    print(f"Testovan√© datab√°ze: {', '.join(databases_to_test)}")
    
    for db in databases_to_test:
        for users in concurrency_levels:
            start_wall_clock = time.time()
            
            stats = run_benchmark(db, users)
            
            end_wall_clock = time.time()
            total_duration = end_wall_clock - start_wall_clock
            
            if stats:
                # P≈ôid√°me throughput (Queries Per Second)
                stats["TPS"] = round(stats["Total_Queries"] / total_duration, 2)
                results.append(stats)
                print(f"   üëâ TPS: {stats['TPS']} | Avg Latency: {stats['Avg_Latency_s']}s\n")
            
            # Pauza mezi testy
            time.sleep(2)

    # Naƒçten√≠ existuj√≠c√≠ch v√Ωsledk≈Ø, pokud existuj√≠, pro kombinovan√Ω graf
    if os.path.exists(csv_file):
        try:
            existing_df = pd.read_csv(csv_file)
            # Odstran√≠me star√© v√Ωsledky pro pr√°vƒõ testovan√© datab√°ze, abychom je nahradili nov√Ωmi
            if not existing_df.empty:
                existing_df = existing_df[~existing_df['Database'].isin(databases_to_test)]
            
            # Spoj√≠me star√© a nov√© v√Ωsledky
            new_df = pd.DataFrame(results)
            final_df = pd.concat([existing_df, new_df], ignore_index=True)
        except Exception as e:
            print(f"‚ö†Ô∏è Chyba p≈ôi ƒçten√≠ existuj√≠c√≠ho CSV: {e}. Vytv√°≈ô√≠m nov√©.")
            final_df = pd.DataFrame(results)
    else:
        final_df = pd.DataFrame(results)

    print("\nüìä V√ùSLEDKY (Aktualizovan√©):")
    print(final_df)
    
    final_df.to_csv(csv_file, index=False)
    print(f"\n‚úÖ Ulo≈æeno do {csv_file}")
    
    # Vykreslen√≠ grafu ze v≈°ech dat (i tƒõch z p≈ôedchoz√≠ch bƒõh≈Ø)
    if not final_df.empty:
      # Vykreslen√≠ grafu
    try:
        
        # Nastaven√≠ stylu
        sns.set_theme(style="whitegrid")
        plt.figure(figsize=(12, 7))

        # Definice barev podle zad√°n√≠ u≈æivatele (nejefektivnƒõj≈°√≠ verze)
        custom_colors = {
            "ClickHouse": "#4BC0C0",  # Teal
            "MSSQL": "#A0A2A8",       # Grey (MSSQL Col)
            "PostgreSQL": "#FF9F40",  # Orange
            "MariaDB": "#FF6384",     # Pink/Red (MariaDB Col)
            "TimescaleDB": "#FFCD56"  # Yellow
        }
        
        # Pokud by v CSV byly jin√© n√°zvy, fallback na default paletu
        palette = custom_colors if all(db in custom_colors for db in final_df['Database'].unique()) else "viridis"

        # Vytvo≈ôen√≠ barplotu
        chart = sns.barplot(
            data=final_df, 
            x="Concurrency", 
            y="TPS", 
            hue="Database", 
            palette=palette,
            edgecolor="black",
            linewidth=1
        )

        # Popisky a titulek
        plt.title("Propustnost datab√°z√≠ p≈ôi soubƒõ≈æn√© z√°tƒõ≈æi (TPS)", fontsize=16, fontweight='bold', pad=20)
        plt.xlabel("Poƒçet soubƒõ≈æn√Ωch u≈æivatel≈Ø", fontsize=12, labelpad=10)
        plt.ylabel("Transakce za sekundu (TPS) - Vy≈°≈°√≠ je lep≈°√≠", fontsize=12, labelpad=10)
        
        # Legenda
        plt.legend(title="Datab√°ze", title_fontsize='12', fontsize='11', loc='upper right')

        # P≈ôid√°n√≠ hodnot nad sloupce
        for container in chart.containers:
            chart.bar_label(container, fmt='%.2f', padding=3, fontsize=11, fontweight='bold')

        # Jemn√© doladƒõn√≠
        plt.tight_layout()
        
        plt.savefig("benchmark_chart.png", dpi=300, bbox_inches='tight')
        print("‚úÖ Graf ulo≈æen do benchmark_chart.png")
        
    except ImportError:
        print("‚ö†Ô∏è Knihovna seaborn nen√≠ nainstalov√°na. Generuji z√°kladn√≠ graf.")
        try:
            pivot_df = final_df.pivot(index="Concurrency", columns="Database", values="TPS")
            pivot_df.plot(kind='bar', figsize=(10, 6))
            plt.title("Porovn√°n√≠ propustnosti (TPS) p≈ôi z√°tƒõ≈æi")
            plt.ylabel("Transakce za sekundu (TPS)")
            plt.xlabel("Poƒçet soubƒõ≈æn√Ωch u≈æivatel≈Ø")
            plt.grid(axis='y')
            plt.tight_layout()
            plt.savefig("benchmark_chart.png")
            print("‚úÖ Graf ulo≈æen do benchmark_chart.png")
        except Exception as e:
            print(f"‚ö†Ô∏è Nepoda≈ôilo se vykreslit graf: {e}")
    except Exception as e:
        print(f"‚ö†Ô∏è Chyba p≈ôi generov√°n√≠ grafu: {e}")

if __name__ == "__main__":
    main()
