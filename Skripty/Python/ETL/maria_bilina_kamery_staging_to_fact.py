import traceback
import time
import mariadb
from datetime import datetime
import signal
import csv
import tempfile
import os
import sys

# Nastaven√≠
LIMIT_ROWS = 99999999999
BATCH_SIZE = 500000

stop_requested = False
def signal_handler(sig, frame):
    global stop_requested
    stop_requested = True
    print("\nüõë P≈ôeru≈°en√≠ detekov√°no. Dokonƒçuji aktu√°ln√≠ d√°vku a ukonƒçuji skript...")

signal.signal(signal.SIGINT, signal_handler)

def get_db_connection():
    return mariadb.connect(
        host="localhost",
        port=3308,
        user="admin",
        password="C0lumnStore!",
        database="mttgueries",
        autocommit=False,
        local_infile=True # Nutn√© pro LOAD DATA LOCAL INFILE
    )

# --- ZRU≈†EN√ç T≈ò√çDY DimensionCache ---

def insert_dimensions_bulk_set_based(cursor, staging_data):
    """
    Sada-orientovan√© vkl√°d√°n√≠ dimenz√≠ (efektivn√≠ n√°hrada INSERT IGNORE).
    """
    print(f"üìä Vkl√°d√°m nov√© dimenze pomoc√≠ INSERT SELECT LEFT JOIN...")
    
    # 1. Pou≈æijeme doƒçasnou tabulku pro unik√°tn√≠ hodnoty v d√°vce
    cursor.execute("DROP TEMPORARY TABLE IF EXISTS TempDimensionsBulk;")
    cursor.execute("""
        CREATE TEMPORARY TABLE TempDimensionsBulk (
            City VARCHAR(255),
            Sensor VARCHAR(255),
            LP VARCHAR(50),
            DetectionType VARCHAR(50),
            VehClass VARCHAR(50),
            Country VARCHAR(50)
        );
    """)

    # P≈ôiprav√≠me data k vlo≈æen√≠ do doƒçasn√© tabulky (Python zpracov√°v√° jen unik√°ty pro tuto d√°vku)
    cities = set(row[2] for row in staging_data if row[2])
    sensors = set(row[6] for row in staging_data if row[6])
    lps = set(row[5] for row in staging_data if row[5])
    detections = set(row[3] for row in staging_data if row[3])
    vehicles = set(row[7] for row in staging_data if row[7] is not None)
    countries = set(row[8] for row in staging_data if row[8])

    # Sjednot√≠me data a vlo≈æ√≠me do doƒçasn√© tabulky
    temp_data = []
    max_len = max(len(cities), len(sensors), len(lps), len(detections), len(vehicles), len(countries))

    # Tato ƒç√°st je neefektivn√≠ v Pythonu, pokud se dƒõl√° pro KA≈ΩDOU DIMENZI zvl√°≈°≈•.
    # Proto se pou≈æije jen jedna doƒçasn√° tabulka pro v≈°echny dimenze (zjednodu≈°en√Ω p≈ô√≠stup)
    
    # Pro jednoduchost se vr√°t√≠me k hromadn√©mu INSERT IGNORE, kter√Ω je pro MariaDB v po≈ô√°dku
    # a je snaz≈°√≠ na √∫dr≈æbu ne≈æ 6x INSERT INTO SELECT LEFT JOIN.
    
    if cities:
        cursor.executemany("INSERT IGNORE INTO DimCity (CityName) VALUES (%s)", [(c,) for c in cities])
    if sensors:
        cursor.executemany("INSERT IGNORE INTO DimSensor (SensorCode) VALUES (%s)", [(s,) for s in sensors])
    if lps:
        cursor.executemany("INSERT IGNORE INTO DimLP (LicensePlate) VALUES (%s)", [(l,) for l in lps])
    if detections:
        cursor.executemany("INSERT IGNORE INTO DimDetectionType (DetectionType) VALUES (%s)", [(d,) for d in detections])
    if vehicles:
        cursor.executemany("INSERT IGNORE INTO DimVehicleClass (VehicleClass) VALUES (%s)", [(v,) for v in vehicles])
    if countries:
        cursor.executemany("INSERT IGNORE INTO DimCountry (CountryCode) VALUES (%s)", [(c,) for c in countries])
    
    print(f" ¬† ‚úî Vlo≈æeno nov√Ωch dimenz√≠.")


def round_time_to_minute(dt):
    """Zaokrouhl√≠ datetime na minutu"""
    if not dt:
        return None
    return dt.replace(second=0, microsecond=0)

def insert_facts_bulk_file(cursor, staging_data, max_key):
    """
    EXTR√âMN√ç OPTIMALIZACE pro ColumnStore: LOAD DATA INFILE
    Tento k√≥d generuje kl√≠ƒç v Pythonu, co≈æ je spr√°vn√© pro ColumnStore.
    """
    print(f"üì• P≈ôipravuji {len(staging_data):,} ≈ô√°dk≈Ø pro bulk insert...")
    start_time = time.time()
    
    # 1. Naƒçteme V≈†ECHNY KL√çƒåE dimenz√≠ P≈òED vkl√°d√°n√≠m fakt≈Ø
    # To je nutn√©, proto≈æe ColumnStore bulk loader NEUM√ç JOINy!
    class DimensionCache:
        def __init__(self, c):
            # Tady naƒç√≠t√°me V≈†ECHNY kl√≠ƒçe, co≈æ je v ColumnStore nutn√©!
            c.execute("SELECT CityKey, CityName FROM DimCity")
            self.city = {row[1]: row[0] for row in c.fetchall()}
            c.execute("SELECT SensorKey, SensorCode FROM DimSensor")
            self.sensor = {row[1]: row[0] for row in c.fetchall()}
            c.execute("SELECT LPKey, LicensePlate FROM DimLP")
            self.lp = {row[1]: row[0] for row in c.fetchall()}
            c.execute("SELECT DetectionTypeKey, DetectionType FROM DimDetectionType")
            self.detection = {row[1]: row[0] for row in c.fetchall()}
            c.execute("SELECT VehicleClassKey, VehicleClass FROM DimVehicleClass")
            self.vehicle = {row[1]: row[0] for row in c.fetchall()}
            c.execute("SELECT CountryKey, CountryCode FROM DimCountry")
            self.country = {row[1]: row[0] for row in c.fetchall()}
            c.execute("SELECT TimeKey, FullDate FROM DimTime")
            self.time = {row[1]: row[0] for row in c.fetchall()}

    cache = DimensionCache(cursor) # Naƒçteme cache jen p≈ôed vkl√°d√°n√≠m fakt≈Ø

    temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, newline='', suffix='.csv')
    csv_path = temp_file.name
    
    try:
        writer = csv.writer(temp_file)
        new_key = max_key
        
        # Zap√≠≈°eme data do CSV
        for row in staging_data:
            new_key += 1
            stg_id, original_time, city, detection, utc, lp, sensor, vehicle, country, velocity = row
            
            # Zaokrouhlen√≠ ƒçasu a Lookup kl√≠ƒç≈Ø
            rounded_time = round_time_to_minute(original_time)
            
            time_key = cache.time.get(rounded_time, -1)
            sensor_key = cache.sensor.get(sensor, -1) if sensor else -1
            detection_key = cache.detection.get(detection, -1) if detection else -1
            lp_key = cache.lp.get(lp, -1) if lp else -1
            country_key = cache.country.get(country, -1) if country else -1
            vehicle_key = cache.vehicle.get(vehicle, -1) if vehicle is not None else -1
            city_key = cache.city.get(city, -1) if city else -1
            
            # Zapsat ≈ô√°dek do CSV (s CameraDetectionKey)
            writer.writerow([
                new_key, time_key, sensor_key, detection_key, lp_key,
                country_key, vehicle_key, city_key, velocity if velocity is not None else '\\N'
            ])
        
        temp_file.close()
        
        # BULK INSERT p≈ôes LOAD DATA INFILE
        print(f" ¬† ‚û§ Spou≈°t√≠m LOAD DATA INFILE (ColumnStore bulk insert)...")
        
        csv_path_escaped = csv_path.replace('\\', '/')
        
        load_sql = f"""
            LOAD DATA LOCAL INFILE '{csv_path_escaped}'
            INTO TABLE FactCameraDetection
            FIELDS TERMINATED BY ',' 
            LINES TERMINATED BY '\n'
            (CameraDetectionKey, TimeKey, SensorKey, DetectionTypeKey, LPKey, 
             CountryKey, VehicleClassKey, CityKey, @velocity)
            SET Velocity = NULLIF(@velocity, '\\\\N')
        """
        
        cursor.execute(load_sql)
        inserted_count = cursor.rowcount
        
        duration = round(time.time() - start_time, 2)
        print(f"‚úÖ Fakta vlo≈æena za {duration}s ({inserted_count:,} ≈ô√°dk≈Ø) - {inserted_count/duration:,.0f} ≈ô√°dk≈Ø/s")
        
        return inserted_count, new_key
        
    finally:
        try:
            os.unlink(csv_path)
        except:
            pass

def process_batch(batch):
    current_id, batch_end = batch
    conn = get_db_connection()
    cursor = conn.cursor()
    try:
        print(f"\n‚û°Ô∏è ¬†Zpracov√°v√°m d√°vku: LandingID {current_id} a≈æ {batch_end}")
        
        # Naƒçteme staging data
        print(" ¬† ‚û§ Naƒç√≠t√°m data ze stagingu...")
        cursor.execute("""
            SELECT StgID, OriginalTime, City, DetectionType, Utc, LP, 
                         Sensor, VehClass, ILPC, Velocity
            FROM Stg_CameraCamea
            WHERE LandingID BETWEEN %s AND %s
            ORDER BY StgID
        """, (current_id, batch_end))
        
        staging_data = cursor.fetchall()
        print(f" ¬† ‚úî Naƒçteno {len(staging_data):,} ≈ô√°dk≈Ø")
        
        if not staging_data:
            print(" ¬† ‚ö†Ô∏è ≈Ω√°dn√° data k zpracov√°n√≠")
            return 0, batch_end
        
        # Vlo≈æ√≠me nov√© dimenze (Sada-orientovan√Ω INSERT IGNORE je OK)
        insert_dimensions_bulk_set_based(cursor, staging_data)
        conn.commit() # Commit dimenz√≠
        
        # Z√≠sk√°me maxim√°ln√≠ kl√≠ƒç pro ruƒçn√≠ generov√°n√≠
        cursor.execute("SELECT COALESCE(MAX(CameraDetectionKey), 0) FROM FactCameraDetection")
        max_key = cursor.fetchone()[0]
        
        # Vlo≈æ√≠me fakta p≈ôes LOAD DATA INFILE (s ruƒçnƒõ generovan√Ωm kl√≠ƒçem)
        inserted_count, new_max_key = insert_facts_bulk_file(cursor, staging_data, max_key)
        conn.commit() # Commit fakt≈Ø
        
        print(f"üéØ D√°vka {current_id}-{batch_end} √∫spƒõ≈°nƒõ dokonƒçena.")
        return inserted_count, batch_end
        
    except Exception as e:
        conn.rollback()
        print(f"‚ùå Chyba p≈ôi zpracov√°n√≠ d√°vky {current_id}-{batch_end}: {str(e)}")
        print(traceback.format_exc()) 
        return 0, current_id
    finally:
        cursor.close()
        conn.close()

# Funkce main() z≈Øst√°v√° beze zmƒõny

def main():
    conn = None
    cursor = None
    start_time = datetime.now()
    total_inserted = 0
    status = "FAILED"
    error_message = None

    try:
        conn = get_db_connection()
        cursor = conn.cursor()

        # Z√≠sk√°n√≠ max. ID ze Stagingu
        cursor.execute("""
            SELECT COALESCE(MAX(LastLoadedID), 0)
            FROM ETL_IncrementalControl
            WHERE Topic LIKE '/Bilina/kamery/camea/%%';
        """)
        max_id = cursor.fetchone()[0] or 0

        # Z√≠sk√°n√≠ posledn√≠ho zpracovan√©ho ID pro Fact
        cursor.execute("""
            SELECT COALESCE(MAX(LastLoadedID), 0)
            FROM ETL_IncrementalControl
            WHERE Topic = '/Bilina/kamery/staging_to_fact';
        """)
        last_loaded_id = cursor.fetchone()[0] or 0

        if last_loaded_id == 0:
            cursor.execute("SELECT COALESCE(MIN(LandingID), 0) FROM Stg_CameraCamea;")
            last_loaded_id = cursor.fetchone()[0] or 0

        current_id = last_loaded_id
        batch_limit = min(current_id + LIMIT_ROWS - 1, max_id)
        
        if batch_limit <= current_id:
            print(f"\n‚úÖ ETL ji≈æ probƒõhlo, nebo nejsou k dispozici nov√° data.")
            status = "SUCCESS"
            return

        print(f"\nüöÄ Zpracov√°n√≠ z√°znam≈Ø s LandingID od {current_id:,} do {batch_limit:,} (d√°vky: {BATCH_SIZE:,})...\n")

        batches = [
            (i, min(i + BATCH_SIZE - 1, batch_limit)) 
            for i in range(current_id, batch_limit + 1, BATCH_SIZE)
        ]
        print(f"üîÅ P≈ôipraveno d√°vek: {len(batches)}\n")
        
        last_processed_id = current_id

        for idx, batch in enumerate(batches, 1):
            if stop_requested:
                print("‚èπ P≈ôeru≈°en√≠ u≈æivatelem potvrzeno. ETL se ukonƒçuje...")
                break
            
            elapsed = (datetime.now() - start_time).total_seconds()
            rate = total_inserted / elapsed if elapsed > 0 else 0
            remaining_rows = batch_limit - last_processed_id
            eta = remaining_rows / rate if rate > 0 else 0
            
            print(f"[{idx}/{len(batches)}] Celkov√° rychlost: {rate:,.0f} ≈ô√°dk≈Ø/s | ETA: {eta/60:.1f} min")
            
            inserted, new_id = process_batch(batch)
            total_inserted += inserted

            if new_id > last_processed_id:
                last_processed_id = new_id

                cursor.execute("""
                    INSERT INTO ETL_IncrementalControl (Topic, LastLoadedID, FullLoadDone, LastUpdate, ProcessStep)
                    VALUES ('/Bilina/kamery/staging_to_fact', %s, 0, NOW(), 1)
                    ON DUPLICATE KEY UPDATE LastLoadedID = %s, LastUpdate = NOW(), ProcessStep = 1;
                """, (last_processed_id, last_processed_id))
                conn.commit()
                
        status = "SUCCESS"
        duration = (datetime.now() - start_time).total_seconds()
        print(f"\n‚úÖ ETL dokonƒçeno za {duration/60:.1f} min. Celkem vlo≈æeno: {total_inserted:,} ≈ô√°dk≈Ø.")
        print(f" ¬† Pr≈Ømƒõrn√° rychlost: {total_inserted/duration:,.0f} ≈ô√°dk≈Ø/s")

    except KeyboardInterrupt:
        print("\n‚õîÔ∏è ETL proces byl p≈ôeru≈°en u≈æivatelem (Ctrl+C)")
        error_message = "ETL p≈ôeru≈°eno u≈æivatelem"
        status = "FAILED"
    except Exception as e:
        error_message = traceback.format_exc()
        print(f"‚ùå Chyba ETL procesu: {str(e)}")
        print(error_message)

    finally:
        try:
            if conn and cursor:
                end_time = datetime.now()
                log_message = error_message if status != "SUCCESS" else None
                rows_log = total_inserted if status == "SUCCESS" else None
                
                cursor.execute("""
                    INSERT INTO ETL_RunLog (JobName, Topic, Status, StartTime, EndTime, RowsInserted, ErrorMessage)
                    VALUES ('Load_FactCameraDetection', '/Bilina/kamery/staging_to_fact', %s, %s, %s, %s, %s);
                """, (status, start_time, end_time, rows_log, log_message))
                conn.commit()
        except Exception as log_err:
            print(f"‚ö†Ô∏è Chyba p≈ôi logov√°n√≠: {log_err}", file=sys.stderr)
        finally:
            if cursor:
                cursor.close()
            if conn:
                conn.close()

if __name__ == "__main__":
    main()